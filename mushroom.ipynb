{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Integrador - Redes Neuronales.\n",
    "\n",
    "Clasificación para determinar si un hongo es comestible o no teniendo en cuenta distintas caracteristicas fisicas del mismo.\n",
    "\n",
    "\n",
    "data set: https://www.kaggle.com/datasets/prishasawhney/mushroom-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura y analisis del CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('mushroom.csv')\n",
    "csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv.fillna(csv.median(), inplace=True)\n",
    "csv.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por medio del histograma podemos ver que hay valores atipicos en la columna 'gill-color'\n",
    "\n",
    "Es por esto que se descarta un 10% de cada extremo luego de ordenar el csv por esa columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.corr()\n",
    "csv.hist(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_sorted = csv.sort_values(by='gill-color')\n",
    "\n",
    "n_rows = len(csv_sorted)\n",
    "\n",
    "ten_percent = int(0.10 * n_rows)\n",
    "\n",
    "csv_trimmed = csv_sorted.iloc[ten_percent:-ten_percent]\n",
    "\n",
    "csv_trimmed.to_csv('mushroom_trimmed.csv', index=False)\n",
    "\n",
    "csv = pd.read_csv('mushroom_trimmed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacion de los datos despues de descartar los valores atipicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = csv.drop('class', axis=1)\n",
    "crop = csv['class']\n",
    "csv_stats = csv_data.describe().T\n",
    "csv_data_n = (csv_data - csv_stats['mean']) / csv_stats['std']\n",
    "csv_n = pd.concat([csv_data_n, crop], axis=1)\n",
    "csv_n.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.4\n",
    "x = np.arange(len(csv.columns))\n",
    "\n",
    "ax1.bar(x, csv.describe().loc['mean'], bar_width, color='green', label='Media')\n",
    "ax1.set_xlabel('Características')\n",
    "ax1.set_ylabel('Valor Medio', color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(csv.columns, rotation=45)\n",
    "ax1.set_title('Valores Medios y Desvío Estándar Antes De Normalizar')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, csv.describe().loc['std'], color='blue', marker='o', label='Desvío Estándar')\n",
    "ax2.set_ylabel('Desvío Estándar', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(csv_data_n.columns, csv_data_n.describe().loc['mean'], color='red')\n",
    "plt.title('Valores Medios Post Normalizar')\n",
    "plt.xlabel('Características')\n",
    "plt.ylabel('Valor Medio')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(csv.corr() ,annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division de los datos para la red neuronal y para usar con Scikit Learn\n",
    "\n",
    "Se utilizo un 70% de la muestra para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = csv_n.drop(['class'],axis=1)\n",
    "\n",
    "X = X_drop.values\n",
    "Y = csv_n['class'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Se utilizaron dos capas ocultas de 20 neuronas cada una.\n",
    "\n",
    "La funcion de activacion es ReLu y la funcion de salida es Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_size = X_train.shape[1] # Número de características de entrada\n",
    "hidden1_size = 20  # Tamaño de la primera capa oculta\n",
    "hidden2_size = 20 # Tamaño de la segunda capa oculta\n",
    "output_size = 1  # Tamaño de la capa de salida\n",
    "\n",
    "# Pesos\n",
    "w_hidden1 = np.random.rand(hidden1_size, input_size)\n",
    "w_hidden2 = np.random.rand(hidden2_size, hidden1_size)\n",
    "w_output = np.random.rand(output_size, hidden2_size)\n",
    "# Sesgos\n",
    "b_hidden1 = np.random.rand(hidden1_size, 1)\n",
    "b_hidden2 = np.random.rand(hidden2_size, 1)\n",
    "b_output = np.random.rand(output_size, 1)\n",
    "\n",
    "# Funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "#softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivadas de las funciones de activación\n",
    "d_relu = lambda x: x > 0\n",
    "d_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "# Función que ejecuta la propagación hacia adelante de la red neuronal\n",
    "def forward_prop(X):\n",
    "    Z1 = w_hidden1 @ X + b_hidden1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = w_output @ A2 + b_output\n",
    "    A3 = logistic(Z3) * 21 \n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[-1]\n",
    "test_comparisons = np.equal((test_predictions >= .5).astype(int), Y_test)\n",
    "accuracy = np.mean(test_comparisons.astype(int))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Se configuro con 100000 iteraciones y una tasa de aprendizaje del 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = .0001  # La tasa de aprendizaje\n",
    "I = 100_000 # Cant de iteraciones\n",
    "\n",
    "# Devuelve pendientes para pesos y sesgos\n",
    "# usando la regla de la cadena\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 = 2*A3 - 2*Y\n",
    "    dA3_dZ3 = d_logistic(Z3)\n",
    "    dZ3_dA2 = w_output\n",
    "    dZ3_dW3 = A2\n",
    "    dZ3_dB3 = 1\n",
    "    dA2_dZ2 = d_relu(Z2)\n",
    "    dZ2_dA1 = w_hidden2\n",
    "    dZ2_dW2 = A1\n",
    "    dZ2_dB2 = 1\n",
    "    dA1_dZ1 = d_relu(Z1)\n",
    "    dZ1_dW1 = X\n",
    "    dZ1_dB1 = 1\n",
    "\n",
    "    dC_dW3 = dC_dA3 @ dA3_dZ3 @ dZ3_dW3.T\n",
    "\n",
    "    dC_dB3 = dC_dA3 @ dA3_dZ3 * dZ3_dB3\n",
    "\n",
    "    dC_dA2 = dC_dA3 @ dA3_dZ3 @ dZ3_dA2\n",
    "\n",
    "    dC_dW2 = dC_dA2 @ dA2_dZ2 @ dZ2_dW2.T\n",
    "\n",
    "    dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2\n",
    "\n",
    "    dC_dA1 = dC_dA2 @ dZ2_dA1\n",
    "\n",
    "    dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T\n",
    "\n",
    "    dC_dB1 = dC_dA1 @ dA1_dZ1 * dZ1_dB1\n",
    "\n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "# Ejecutar descenso de gradiente\n",
    "n = X_train.shape[0]\n",
    "print(n)\n",
    "for i in range(I):\n",
    "    # seleccionar aleatoriamente un conjunto de datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx]\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # actualizar pesos y sesgos\n",
    "    w_hidden1 -= L * dC_dW1\n",
    "    b_hidden1 -= L * dC_dB1\n",
    "    w_hidden2 -= L * dC_dW2\n",
    "    b_hidden2 -= L * dC_dB2\n",
    "    w_output -= L * dC_dW3\n",
    "    b_output -= L * dC_dB3\n",
    "\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[3]\n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparacion con Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(\n",
    "\t                solver='sgd', \n",
    "                    hidden_layer_sizes=(20, 20),\n",
    "                    activation='relu',\n",
    "                    max_iter=100_000,\n",
    "                    learning_rate_init=.0001,\n",
    "                    verbose=True,\n",
    "                    random_state=42,\n",
    "                    tol=1e-6)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "\n",
    "print(\"Puntaje del conjunto de entrenamiento: %f\" % model.score(X_train, Y_train))\n",
    "print(\"Puntaje del conjunto de prueba: %f\" % model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo hecho con Scikit Learn resulto tener mucha mas exactitud que el modelo manual.\n",
    "\n",
    "Intente buscar en la red donde se puede estar perdiendo ese ~50% de exactitud pero no logre dar con el error en el codigo.\n",
    "\n",
    "Asumo que es un error en la parte de Backpropagation ya que en el modelo de Scikit Learn se usaron las mismas configuraciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
